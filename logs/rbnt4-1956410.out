/neuro/labs/grantlab/users/arafat.hussain/bnt/lib64/python3.6/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
[2024-07-16 17:28:04,142][HYDRA] Launching 1 jobs locally
[2024-07-16 17:28:04,142][HYDRA] 	#0 : model=rbnt4 score=fiq
wandb: Currently logged in as: mhdarafat. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/wandb/run-20240716_172805-hk5i60mv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-wind-215
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mhdarafat/brainnetworktransformer
wandb: üöÄ View run at https://wandb.ai/mhdarafat/brainnetworktransformer/runs/hk5i60mv
/neuro/labs/grantlab/users/arafat.hussain/bnt/lib64/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
[2024-07-16 17:28:13,613][optimizer.py][L65][INFO] Parameters [normal] length [51]
[2024-07-16 17:28:13,615][training.py][L35][INFO] #model params: 4782164
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: üöÄ View run worldly-wind-215 at: https://wandb.ai/mhdarafat/brainnetworktransformer/runs/hk5i60mv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240716_172805-hk5i60mv/logs
Error executing job with overrides: ['model=rbnt4', 'score=fiq']
Traceback (most recent call last):
  File "/neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/source/__main__.py", line 71, in main
    model_training(cfg)
  File "/neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/source/__main__.py", line 57, in model_training
    training.train()
  File "/neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/source/training/training.py", line 406, in train
    self.train_pairs_per_epoch2(self.config, self.train_dataloader, self.optimizers[0], self.lr_schedulers[0])
  File "/neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/source/training/training.py", line 339, in train_pairs_per_epoch2
    predict = self.model(node_feature1, node_feature2)
  File "/neuro/labs/grantlab/users/arafat.hussain/bnt/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/neuro/labs/grantlab/users/arafat.hussain/RelationalConnectivityTransformer/source/models/RBNT/rbnt4.py", line 115, in forward
    for atten1 in self.attention_list1:
  File "/neuro/labs/grantlab/users/arafat.hussain/bnt/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 1178, in __getattr__
    type(self).__name__, name))
AttributeError: 'RelationalBrainNetworkTransformer4' object has no attribute 'attention_list1'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
